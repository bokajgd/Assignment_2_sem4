---
title: "Computational Modeling - Assignment 2"
author: "Riccardo Fusaroli"
date: "29/01/2019"
output: html_document
editor_options: 
  chunk_output_type: inline
---

1. Solve the task using grid approximation
2. Use a uniform prior
3. Calculate a posterior
4. Plot the results (containing both the prior and the posterior - even though the prior is flat)

5. Repeat above task using a quadratic approximation to investigate how the method changes the results

6. Apply the pipeline to analyze results for all teachers

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci

N.B. there is a second part at the bottom for next week.

### First part

You want to assess your teachers' knowledge of cognitive science. "These guys are a bunch of drama(turgist) queens, mindless philosophers, chattering communication people and Russian spies. Do they really know CogSci?", you think.

To keep things simple (your teachers should not be faced with too complicated things):
- You created a pool of equally challenging questions on CogSci
- Each question can be answered correctly or not (we don't allow partially correct answers, to make our life simpler).
- Knowledge of CogSci can be measured on a scale from 0 (negative knowledge, all answers wrong) through 0.5 (random chance) to 1 (awesome CogSci superpowers)

This is the data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

```{r Loading libraries}
library(pacman)
p_load(rethinking, tidyverse, rstan, brms, dplyr, source)

source("~/Desktop/Cognitive Science/Cognitive Science 4th semester/Computational Modeling for Cognitive Science/Computational_modeling/my_useful_functions_class1.R")


# create a tibble from the data
all_df <- tibble(
  person = c('RF', 'KT', 'JS', 'MW'),
  correct = c(3, 2, 160, 66),
  answered = c(6, 2, 198, 132))
```


Questions:

# 1. What's Riccardo's estimated knowledge of CogSci? 
What is the probability he knows more than chance (0.5) [try figuring this out. if you can't peek into chapters 3.1 and 3.2 and/or the slides]?
- First implement a grid approximation (hint check paragraph 2.4.1!) with a uniform prior, calculate the posterior and plot the results
- Then implement a quadratic approximation (hint check paragraph 2.4.2!).
- N.B. for the rest of the exercise just keep using the grid approximation (we'll move to quadratic approximations in two classes)

```{r Riccardo's estimated knowledge of CogSci based on grid approximation}
# defining a probability grid
dens <- 1e4
p_grid <- seq(from = 0, to = 1, length.out = dens)
# calculate bin size
bin_size <- p_grid[2]
 # defining a normal distribution with mean 0.5 and sd of 0.2 as the prior
prior <- dnorm(p_grid, mean = 0.5, sd = 0.2)
# defining the likelihood of a correct response
likelihood <- dbinom(3, size=6, prob = p_grid)
# computing the standardized posterior
posterior <- likelihood * prior / sum(likelihood * bin_size * prior)
# plotting Riccardo's posterior
plot(p_grid, posterior)
# creating a tibble containing the data
d <- tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior)
# plotting the posterior distribution of Riccardo's ability
ggplot(d)+
  geom_line(aes(p_grid, posterior), color="red")+
  geom_line(aes(p_grid, prior), color="blue")+
  annotate("text", x=0.635, y=2.6, label= "Posterior", colour = "red")+ 
  annotate("text", x=0.85, y=.7, label= "Prior", colour = "blue")+ 
  theme_classic()+
  labs(y=NULL, x="Grid")
  #geom_line(aes(p_grid, prior), color="c")
# calculating the parts of the distribution with maximum probability by drawing samples from the posterior distribution
samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
HPDI(samples)
# Calculating the percent chance that Riccardo will score above chance
sum((samples >= 0.5) == TRUE) * bin_size

sd(samples)
```

##

```{r Riccardo's estimated knowledge of CogSci based on quadratic approximation}

# f is the binomial distribution (c) with a uniform prior (p)
ric_performance <- rethinking::map(
  alist(c ~ dbinom(6, p), # binomial distribution
        p ~ dnorm(0.5,0.2)), # uniform prior
        data = list(c = 3)
  )
# summary of the quadratic approximation
precis(ric_performance)
# specify a normal distribution based on the output of the model
ric_dist <- dnorm(p_grid, mean = 0.5, sd = 0.14)
# adding riccardos distribution to the tibble 'd'
d$ric_dist <- ric_dist
# plotting the quadratic approximation
ggplot(d)+
  geom_line(aes(p_grid, ric_dist), color = "red")+
  geom_line(aes(p_grid, prior), color = "black")

samples_quad <- sample(p_grid, size = dens, replace = TRUE, prob = ric_dist)
sum((samples_quad >= 0.5) == TRUE) / length(samples_quad)

```



# 2. Estimate all the teachers' knowledge of CogSci. Who's best? 
Use grid approximation. Comment on the posteriors of Riccardo and Mikkel.
2a. Produce plots of the prior, and posterior for each teacher.

```{r Calculating a posterior for each teacher}
dens <- 1e4 # set a density value of 1000
p_grid <- seq(from = 0, to = 1, length = dens) # defining a probability grid for grid approximation
prior <- dnorm(p_grid, mean = 0.5, sd = 0.2)  # defining a uniform prior

for (i in 1:length(all_d$person)){
  likelihood <- dbinom(all_d$correct[i], size = all_d$answered[i], prob = p_grid)
  posterior <- likelihood * prior / sum(likelihood * prior * bin_size)
  # creating a tibble containing the data
  d <- tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior)
  # drawing a sample from the posterior distribution
  samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
  # creating a rounded list of the sample HDPI-values
  x <- round(HPDI(samples), 3)
  # getting the probability of scoring above chance.
  xx <- sum((samples > 0.5) == TRUE) / length(samples)
  # plotting the posterior distribution of all teachers knowledge of cogsci
  plot <- ggplot(d)+
    geom_line(aes(p_grid, posterior), color = "red")+
    geom_line(aes(p_grid, prior), color = "blue")+
    theme_classic()+
    labs(x = "Probability grid",
         y = "Posterior density",
         title = all_d$person[i],
         subtitle = paste("|89 -", x[1], "   ", x[2], "- 89|", "   Scoring > chance:", xx*100, "%", sep = " "))
  print(plot)
  print(paste("Teacher:", all_d$person[i], "HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   Scoring > chance:", xx, sep = " "))
}
```
The above code prints out 4 plots containing both prior and posterior for every person we gathered data on, in this case our 4 teachers; Riccardo, Kristian, Josh & Mikkel. The plots furthermore contain information about the HPDI-value for each probability distribution.
Since we have used a uniform prior for every individual there is not much interesting to say about this. We do however see a major difference in the posteriors. 
- Riccardo's knowledge of cogsci produces a rather Gaussian-looking distribution with quite large variance due to the low samplesize. This is also illustrated by the 89th percentiles which are 0.23 and 0.76.
- Kristian's knowledge of cogsci produces a distribution indicative of no possibility of getting none correct and a high probabiliy of getting everything correct. The percentiles we can see that the bulk of the distribution lies between 0.48 and 1. the bottom percentile is most likely as low as it is due to the uniform prior.
- Josh's knowedlge of cogsci produces a very condensed probability function in the percentiles from 0.76 0.85. We can see that Josh is a lot better than e.g. Riccardo and we also has a much more condensed graph because of the fact that we have many more samples to base our posterior distribution on.
- Mikkel's knowledge of cogsci seems to be about the same as Riccardo's, we can however judge this with more certainty as we have more data. More data produces a more precise posterior and the percentiles vary from 0.43 - 0.57.
From investigating the possibility of scoring > chance we can see that Josh is the most knowledgeable cognitive science teacher and that Riccardo and Mikkel are almost exactly equally bad.


# 3. Change the prior. Given your teachers have all CogSci jobs, you should start with a higher appreciation of their knowledge:
the prior is a normal distribution with a mean of 0.8 and a standard deviation of 0.2. Do the results change (and if so how)?
3a. Produce plots of the prior and posterior for each teacher.

```{r Calculating posterior based on normally distributed prior}
dens <- 1e4 # set a density value of 1000
p_grid <- seq(from = 0, to = 1, length = dens) # defining a probability grid for grid approximation
prior <- dnorm(p_grid, mean = 0.8, sd = 0.2) # defining a normal distribution with mean 0.8 and sd of 0.2 as the prior

for (i in 1:length(all_d$person)){
  likelihood <- dbinom(all_d$correct[i], size = all_d$answered[i], prob = p_grid)
  posterior <- likelihood * prior / sum(likelihood * bin_size * prior)
  # creating a tibble containing the data
  d <- tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior)
  # drawing a sample from the posterior distribution
  samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
  # creating a rounded list of the sample HDPI-values
  x <- round(HPDI(samples), 3)
  # getting the probability of scoring above chance.
  xx <- sum((samples > 0.5) == TRUE) / length(samples)
  # plotting the posterior distribution of all teachers knowledge of cogsci
  plot <- ggplot(d)+
    geom_line(aes(p_grid, posterior), color = "red")+
    geom_line(aes(p_grid, prior), color = "blue")+
    labs(x = "probability grid",
         y = "posterior density",
         title = all_d$person[i],
         subtitle = paste("HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx*100, "%", sep = " "))
  print(plot)
  print(paste("Teacher:", all_d$person[i], "HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx, sep = " "))
}
```
Changing the prior changes the outrcome of the posterior distribution a lot for the individuals with very low response rate and a little for individuals with a higher response-rate. As an example Riccardo goes from having 50% chance of scoring > chance with a uniform prior to having an 84% chance of scoring > chance with a normally distributed prior with a mean of 0.8 and a sd of 0.2. On the other hand, Mikkel - who had the same chance of scoring > chance as Riccardo with a uniform prior - only improves to the point of having a 63% probability of scoring above chance.



# 4. You go back to your teachers and collect more data (multiply the previous numbers by 100). 
Calculate their knowledge with both a uniform prior and a normal prior with a mean of 0.8 and a standard deviation of 0.2. Do you still see a difference between the results? Why?

```{r Recalculating posteriors with more data}
new_d <- all_d
new_d$correct <- all_d$correct * 100
new_d$answered <- all_d$answered * 100

# Calculating the posterior for each teacher with uniform prior
dens <- 1e4 # set a density value of 1000
p_grid <- seq(from = 0, to = 1, length = dens) # defining a probability grid for grid approximation
prior <- rep(1, dens) # defining a uniform prior

for (i in 1:length(all_d$person)){
  likelihood <- dbinom(new_d$correct[i], size = new_d$answered[i], prob = p_grid)
  posterior <- likelihood * prior / sum(likelihood * bin_size * prior)
  # creating a tibble containing the data
  d <- tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior)
  # drawing a sample from the posterior distribution
  samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
  # creating a rounded list of the sample HDPI-values
  x <- round(HPDI(samples), 3)
  # getting the probability of scoring above chance.
  xx <- sum((samples > 0.5) == TRUE) / length(samples)
  # plotting the posterior distribution of all teachers knowledge of cogsci
  plot <- ggplot(d)+
    geom_line(aes(p_grid, posterior), color = "red")+
    geom_line(aes(p_grid, prior), color = "blue")+
    labs(x = "probability grid",
         y = "posterior density",
         title = all_d$person[i],
         subtitle = paste("|89 -", x[1], "   ", x[2], "- 89|", "   scoring > chance:", xx*100, "%", sep = " "))
  print(plot)
  print(paste("Teacher:", all_d$person[i], "HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx, sep = " "))
}


# Calculating the posterior for each teacher with a gaussian prior with mean = 0.8 and sd = 0.2
dens <- 1e4 # set a density value of 1000
p_grid <- seq(from = 0, to = 1, length = dens) # defining a probability grid for grid approximation
prior <- dnorm(p_grid, mean = 0.8, sd = 0.2) # defining a normal distribution with mean 0.8 and sd of 0.2 as the prior

for (i in 1:length(all_d$person)){
  likelihood <- dbinom(new_d$correct[i], size = new_d$answered[i], prob = p_grid)
  posterior <- likelihood * prior / sum(likelihood * bin_size * prior)
  # creating a tibble containing the data
  d <- tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior)
  # drawing a sample from the posterior distribution
  samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
  # creating a rounded list of the sample HDPI-values
  x <- round(HPDI(samples), 3)
  # getting the probability of scoring above chance.
  xx <- sum((samples > 0.5) == TRUE) / length(samples)
  # plotting the posterior distribution of all teachers knowledge of cogsci
  plot <- ggplot(d)+
    geom_line(aes(p_grid, posterior), color = "red")+
    geom_line(aes(p_grid, prior), color = "blue")+
    labs(x = "probability grid",
         y = "posterior density",
         title = all_d$person[i],
         subtitle = paste("HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx*100, "%", sep = " "))
  print(plot)
  print(paste("Teacher:", all_d$person[i], "HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx, sep = " "))
}
```
When the sample size is larger we see a smaller change in the posterior distribution even though we change the prior to be gaussian with a mean of 0.8. This is because the evidence weighs more the larger the sample size, and in this case the evidence given by the actual responses is so overwhelming that the prior expectations don't change too much about the posterior distribution.


# 5. Imagine you're a skeptic and think your teachers do not know anything about CogSci, given the content of their classes.
How would you operationalize that belief?
```{r Recalculating posteriors with a skeptic prior}
# Calculating the posterior for each teacher with a gaussian prior with mean = 0.3 and sd = 0.2
dens <- 1e4 # set a density value of 1000
p_grid <- seq(from = 0, to = 1, length = dens) # defining a probability grid for grid approximation
prior <- dnorm(p_grid, mean = 0.5, sd = 0.2) # defining a normal distribution with mean 0.5 and sd of 0.2 as the prior

for (i in 1:length(all_d$person)){
  likelihood <- dbinom(all_d$correct[i], size = all_d$answered[i], prob = p_grid)
  posterior <- likelihood * prior / sum(likelihood * prior * bin_size)
  # creating and extracting a tibble containing the data for each participant
  nam <- paste("d", all_d$person[i], sep="_") # new name for each participant
  assign(nam, tibble(prior = prior, p_grid = p_grid, likelihood = likelihood, posterior = posterior))
  # drawing a sample from the posterior distribution
  samples <- sample(p_grid, size = dens, replace = TRUE, prob = posterior)
  # creating a rounded list of the sample HDPI-values
  x <- round(HPDI(samples), 3)
  # getting the probability of scoring above chance.
  xx <- sum((samples > 0.5) == TRUE) / length(samples)
  # plotting the posterior distribution of all teachers knowledge of cogsci
  plot <- ggplot(d)+
    geom_line(aes(p_grid, posterior), color = "red")+
    geom_line(aes(p_grid, prior), color = "blue")+
    geom_line(aes(p_grid, likelihood), color = "black")+
    labs(x = "probability grid",
         y = "posterior density",
         title = all_d$person[i],
         subtitle = paste("HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx*100, "%", sep = " "))
  print(plot)
  print(paste("Teacher:", all_d$person[i], "HDPI:", "|89 -", x[1], " ", x[2], "- 89|", "   scoring > chance:", xx, sep = " "))
}
```
A way of modeling the disbelief in our teachers knowledge of cogsci is by adjusting our prior to account for that. We could e.g. set the prior to be a gaussian distribution with a mean of 0.3 and a sd of 0.2. This is quite a skeptic model as we're essentially saying that they'll get about every third answer correct (lower than chance).

# 6. Optional question: 
Can you estimate the difference between Riccardo's estimated knowledge and that of each of the other teachers? Would you deem it credible ($that is, would you believe that it is actually different)?

# 7. Bonus knowledge: 
all the stuff we have done can be implemented in a lme4-like fashion using the brms package. Here is an example.
```{r}
library(brms)

d <- data.frame(
  Correct=c(3,2,160,66),
  Questions=c(6,2,198,132),
  Teacher=c("RF","KT","JS","MW"))

# Model sampling only from the prior (for checking the predictions your prior leads to)
FlatModel_priorCheck <- brm(Correct|trials(Questions) ~ 1, 
                 data = subset(d, Teacher=="RF"),
                 prior = prior("uniform(0,1)", class = "Intercept"),
                 family = binomial,
                 sample_prior = "only") # here we tell the model to ignore the data

# Plotting the predictions of the model (prior only) against the actual data
pp_check(FlatModel_priorCheck, nsamples = 100)

# Model sampling by combining prior and likelihood
FlatModel <- brm(Correct|trials(Questions) ~ 1, 
                 data = subset(d, Teacher=="RF"),
                 prior = prior("uniform(0,1)", class = "Intercept"),
                 family = binomial,
                 sample_prior = T)
# Plotting the predictions of the model (prior + likelihood) against the actual data
pp_check(FlatModel, nsamples = 100)

# plotting the posteriors and the sampling process
plot(FlatModel)


PositiveModel_priorCheck <- brm(Correct|trials(Questions) ~ 1,
                     data = subset(d, Teacher=="RF"),
                     prior = prior("normal(0.8,0.2)", 
                                   class = "Intercept"),
                     family=binomial,
                     sample_prior = "only")
pp_check(PositiveModel_priorCheck, nsamples = 100)

PositiveModel <- brm(Correct|trials(Questions) ~ 1,
                     data = subset(d, Teacher=="RF"),
                     prior = prior("normal(0.8,0.2)", 
                                   class = "Intercept"),
                     family=binomial,
                     sample_prior = T)
pp_check(PositiveModel, nsamples = 100)
plot(PositiveModel)

SkepticalModel_priorCheck <- brm(Correct|trials(Questions) ~ 1, 
                      data = subset(d, Teacher=="RF"),
                      prior=prior("normal(0.5,0.01)", class = "Intercept"),
                      family=binomial,
                      sample_prior = "only")
pp_check(SkepticalModel_priorCheck, nsamples = 100)

SkepticalModel <- brm(Correct|trials(Questions) ~ 1, 
                      data = subset(d, Teacher=="RF"),
                      prior = prior("normal(0.5,0.01)", class = "Intercept"),
                      family = binomial,
                      sample_prior = T)
pp_check(SkepticalModel, nsamples = 100)
plot(SkepticalModel)
```

If you dare, try to tweak the data and model to test two hypotheses:
- Is Kristian different from Josh?
- Is Josh different from chance?
```{r}
p_load(patchwork)
prior <- dnorm(p_grid, mean = 0.5, sd = 0.2) # defining a normal distribution with mean 0.5 and sd of 0.2 as the prior

res_RF <- calc_teacher(n_correct = all_df$correct[all_df$person == "RF"], 
                              n_question = all_df$answered[all_df$person == "RF"], 
                              prior = prior)
# Kristian 
res_MW <- calc_teacher(n_correct = all_df$correct[all_df$person == "MW"], 
                              n_question = all_df$answered[all_df$person == "MW"], 
                              prior = prior)

res_JS <- calc_teacher(n_correct = all_df$correct[all_df$person == "JS"], 
                              n_question = all_df$answered[all_df$person == "JS"], 
                              prior = prior)
# used as an example for Josh
res_KT <- calc_teacher(n_correct = all_df$correct[all_df$person == "KT"], 
                              n_question = all_df$answered[all_df$person == "KT"], 
                              prior = prior)

p <- pretty_plot(p_grid = res_RF$grid, prior = res_RF$prior, likelihood = res_RF$likelihood, posterior = res_RF$teacher_posterior)
p1 <-  pretty_plot(p_grid = res_KT$grid, prior = res_KT$prior, likelihood = res_KT$likelihood, posterior = res_KT$teacher_posterior)
pMW <-  pretty_plot(p_grid = res_MW$grid, prior = res_MW$prior, likelihood = res_MW$likelihood, posterior = res_MW$teacher_posterior)
pJS <-  pretty_plot(p_grid = res_JS$grid, prior = res_JS$prior, likelihood = res_JS$likelihood, posterior = res_JS$teacher_posterior)


# patchwork 
(p + ggtitle("Riccardo")) + (p1 + ggtitle("Kristian"))
(pMW + ggtitle("Mikkel")) + (p1 + ggtitle("Kristian"))
(pJS + ggtitle("Josh")) + (p1 + ggtitle("Kristian"))


RF_pos <- res_RF$teacher_posterior
KT_pos <- res_KT$teacher_posterior
MW_pos <- res_MW$teacher_posterior
JS_pos <- res_JS$teacher_posterior# vector of length == p_grid (10 000)

sam_JS <- sample(size = 100000, x = p_grid, prob = JS_pos, replace = T)
mean(sam_JS)
sd(sam_JS)
sam_KT <- sample(size = 100000, x = p_grid, prob = KT_pos, replace = T)
mean(sam_KT)
sd(sam_KT) # it equivalent to that the SD for KT is bigger than for JS and to say that the variance is bigger for KT that for JS given that variance = sd^2

#Sample for mikkel
sam_MW <- sample(size = 100000, x = p_grid, prob = MW_pos, replace = T)
sam_RF <- sample(size = 100000, x = p_grid, prob = RF_pos, replace = T)

# this give you answer to - (on average) how likely is it that Kristian is smart than mikkel
sum(sam_KT > sam_MW)/100000*100

sum(sam_KT > sam_JS)/100000*100
```


### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. N.B. You can choose which prior to use for the analysis of last year's data.

Questions to be answered (but see guidance below):
1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models
2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions.
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1)
4. How does the new data look in last year's predictive posterior? (way 2)

```{r Calculating new posteriors using 2019 posteriors as priors and doing prediction error testing on post pred dist}
# Test whether new posterior distributions have been credibly altered by new data using old data as priors
new_df <- tibble(
  person = c('RF', 'KT', 'JS', 'MW'),
  correct = c(9, 8, 148, 34),
  answered = c(10, 12, 172, 65))

#___________________________________________ RICCARDO _________________________________________________

new_RF <- calc_teacher(n_correct = new_df$correct[new_df$person == "RF"], 
                              n_question = new_df$answered[new_df$person == "RF"], 
                              prior = res_RF$teacher_posterior)

np <- pretty_plot(p_grid = new_RF$grid, prior = new_RF$prior, likelihood = new_RF$likelihood, posterior = new_RF$teacher_posterior)


# patchwork 
(p + ggtitle("Riccardo - Old")) + (np + ggtitle("Riccardo - New"))
# Test mean prediction error on new data using old data as posterior
nsam_RF <- sample(size = 100000, x = p_grid, prob = new_RF$teacher_posterior, replace = T)

r_b <-sum(nsam_RF > sam_RF)/100000*100


RFpos_pred = rbinom(100000,  # how many times we simluate this
                  size = 10, # how many question he was asked
                  prob = sam_RF # samples of josh knowledge 
                  )
hist(RFpos_pred)
hist(RFpos_pred-9) # prediction error distribution

rmse_r <- paste("RMSE = ", sqrt(mean((RFpos_pred-9))^2),  collapse = "")

RFpos_pred_df  <- data.frame(RFpos_pred)

Ric <- ggplot(RFpos_pred_df, aes(RFpos_pred-9))+
  geom_histogram(binwidth = 1, colour="black", fill="#E69F00")+
  theme_classic()+
  labs(x= "Prediction error", y="Frequency", title = "Riccardo - Prediction error", subtitle = rmse_r)

#  ___________________________________________ JOSH _________________________________________________

new_JS <- calc_teacher(n_correct = new_df$correct[new_df$person == "JS"], 
                              n_question = new_df$answered[new_df$person == "JS"], 
                              prior = res_JS$teacher_posterior)

npJS <- pretty_plot(p_grid = new_JS$grid, prior = new_JS$prior, likelihood = new_JS$likelihood, posterior = new_JS$teacher_posterior)


# patchwork 
(pJS + ggtitle("Josh - Old")) + (npJS + ggtitle("Josh - New"))
# Test mean prediction error on new data using old data as posterior
nsam_JS <- sample(size = 100000, x = p_grid, prob = new_JS$teacher_posterior, replace = T)

j_b <- sum(nsam_JS > sam_JS)/100000*100


JSpos_pred = rbinom(100000,  # how many times we simluate this
                  size = 172, # how many question he was asked
                  prob = sam_JS # samples of josh knowledge 
                  )
hist(JSpos_pred)


rmse_j <- paste("RMSE = ", sqrt(mean((JSpos_pred-148))^2),  collapse = "")

JSpos_pred_df  <- data.frame(JSpos_pred)

Josh <- ggplot(JSpos_pred_df, aes(JSpos_pred-148))+
  geom_histogram(binwidth = 1, colour="black", fill="#E69F00")+
  theme_classic()+
  labs(x= "Prediction error", y="Frequency", title = "Josh - Prediction error", subtitle = rmse_j)

#___________________________________________ KRISTIAN _________________________________________________

new_KT <- calc_teacher(n_correct = new_df$correct[new_df$person == "KT"], 
                              n_question = new_df$answered[new_df$person == "KT"], 
                              prior = res_KT$teacher_posterior)

np1 <- pretty_plot(p_grid = new_KT$grid, prior = new_KT$prior, likelihood = new_KT$likelihood, posterior = new_KT$teacher_posterior)


# patchwork 
(p1 + ggtitle("Kristian - Old")) + (np1 + ggtitle("Kristian - New"))
# Test mean prediction error on new data using old data as posterior
nsam_KT <- sample(size = 100000, x = p_grid, prob = new_KT$teacher_posterior, replace = T)

k_b <-sum(nsam_KT > sam_KT)/100000*100


KTpos_pred = rbinom(100000,  # how many times we simluate this
                  size = 12, # how many question he was asked
                  prob = sam_KT # samples of josh knowledge 
                  )
hist(KTpos_pred)
hist(KTpos_pred-8) # prediction error distribution

rmse_k <- paste("RMSE = ", sqrt(mean((KTpos_pred-8))^2),  collapse = "")

KTpos_pred_df  <- data.frame(KTpos_pred)

Kris <- ggplot(KTpos_pred_df, aes(KTpos_pred-8))+
  geom_histogram(binwidth = 1, colour="black", fill="#E69F00")+
  theme_classic()+
  labs(x= "Prediction error", y="Frequency", title = "Kristian - Prediction error", subtitle = rmse_k)

#  ___________________________________________ JOSH _________________________________________________

new_MW <- calc_teacher(n_correct = new_df$correct[new_df$person == "MW"], 
                              n_question = new_df$answered[new_df$person == "MW"], 
                              prior = res_MW$teacher_posterior)

npMW <- pretty_plot(p_grid = new_MW$grid, prior = new_MW$prior, likelihood = new_MW$likelihood, posterior = new_MW$teacher_posterior)


# patchwork 
(pMW + ggtitle("Mikkel - Old")) + (npMW + ggtitle("Mikkel - New"))
# Test mean prediction error on new data using old data as posterior
nsam_MW <- sample(size = 100000, x = p_grid, prob = new_MW$teacher_posterior, replace = T)

m_b <- sum(nsam_MW > sam_MW)/100000*100


MWpos_pred = rbinom(100000,  # how many times we simluate this
                  size = 65, # how many question he was asked
                  prob = sam_MW # samples of josh knowledge 
                  )
hist(MWpos_pred)
ss
rmse_m <- paste("RMSE = ", sqrt(mean((MWpos_pred-34))^2),  collapse = "")

MWpos_pred_df  <- data.frame(MWpos_pred)

Mikkel <- ggplot(MWpos_pred_df, aes(MWpos_pred-34))+
  geom_histogram(binwidth = 1, colour="black", fill="#E69F00")+
  theme_classic()+s
  labs(x= "Prediction error", y="Frequency", title = "Mikkel - Prediction error", subtitle = rmse_m)

s

#_________ PLOTS ____________

paste("Likelihood of being better acc. new model:", "Riccardo = ", r_b, "%   ", "Josh = ", j_b, "%   ", "Kristian = ", k_b, "%   ", "Mikkel = ", m_b, "%")
(Ric + Josh + Kris + Mikkel)
```